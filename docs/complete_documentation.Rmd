---
title: "Weather Data Analysis and Visualization"
author: "Rohith Nagula Malyala, Sri Guru Datta Pisupati, Sanjeev Varma Katari"
date: "19-05-2024"
output: html_document
---

# Introduction

This document provides an overview of the analysis and visualization of weather data using R. We will cover the steps taken to clean the data, identify extreme values, and visualize the results. The analysis includes various plots to understand temperature trends, distributions, and correlations between different weather attributes.

# Libraries

We begin by loading the necessary libraries:

```{r}
library(dplyr)
library(lubridate)
library(ggplot2)
library(viridis)
library(tidyr)
```



# Data Preprocessing

## Downloading the Data

In this section, we outline the steps taken to download and preprocess the Global Summary of the Day (GSOD) data from the National Centers for Environmental Information (NCEI). The process involves downloading compressed tar.gz files for each year, extracting them, and then organizing the extracted data into CSV files based on individual weather stations.

### Python Script for Downloading and Extracting Data

The following Python script automates the process of downloading and extracting GSOD data files:

```
import requests
import os
import tarfile
from tqdm import tqdm  # For progress bar (optional)
from concurrent.futures import ThreadPoolExecutor

# Function to fetch and parse directory listing for a given year
def fetch_directory_listing(year):
    base_url = f"https://www.ncei.noaa.gov/data/global-summary-of-the-day/archive/{year}.tar.gz"
    return [base_url]

# Function to extract a tar.gz archive and delete the archive after extraction
def extract_tar_file(archive_path, extract_dir):
    try:
        with tarfile.open(archive_path, "r:gz") as tar:
            tar.extractall(path=extract_dir)
        print(f"Extracted: {archive_path}")

        # Delete the archive after extraction
        os.remove(archive_path)
        print(f"Deleted: {archive_path}")
    except Exception as e:
        print(f"Error extracting {archive_path}: {e}")

# Function to download and extract a specific archive
def download_and_extract_archive(archive_url, extract_dir, year):
    archive_name = archive_url.split("/")[-1]
    local_archive_path = os.path.join(extract_dir, archive_name)

    if os.path.exists(local_archive_path):
        print(f"Skipping {archive_name}, already downloaded and extracted.")
        os.remove(local_archive_path)
        print(f"Removed {archive_name}")
        return

    try:
        # Download the archive
        response = requests.get(archive_url, stream=True)
        response.raise_for_status()  # Raise an exception for HTTP errors

        # Display progress bar for download
        with open(local_archive_path, "wb") as file, tqdm(
            desc=f"{year} - {archive_name}",  # Print current year and archive name
            total=int(response.headers.get("content-length", 0)),
            unit="B",
            unit_scale=True,
            unit_divisor=1024,
        ) as bar:
            for data in response.iter_content(chunk_size=1024):
                file.write(data)
                bar.update(len(data))

        print(f"Downloaded: {archive_name}")

        # Extract the downloaded archive
        extract_tar_file(local_archive_path, extract_dir)
    except Exception as e:
        print(f"Error downloading {archive_name}: {e}")

# Main function to parallelize the download and extraction process
def main():
    start_year = 1929
    end_year = 2023
    download_dir = "gsod_archive_parallel"  # Base local directory to store downloaded and extracted archives
    os.makedirs(download_dir, exist_ok=True)

    with ThreadPoolExecutor(max_workers=24) as executor:
        for year in range(start_year, end_year + 1):



            year_dir = os.path.join(download_dir, str(year))
            os.makedirs(year_dir, exist_ok=True)

            try:
                archives_to_download = fetch_directory_listing(year)
                for archive_url in archives_to_download:
                    executor.submit(
                        download_and_extract_archive, archive_url, year_dir, year
                    )
            except Exception as e:
                print(f"Error processing year {year}: {e}")

    print("Download and extraction completed.")

if __name__ == "__main__":
    main()
```

This script performs the following steps:

1. **Fetch Directory Listing**: Constructs the download URL for the tar.gz file for each year.
2. **Download and Extract Archives**: Downloads the tar.gz file for each year and extracts its contents. It also deletes the tar.gz file after extraction to save space.
3. **Parallel Execution**: Utilizes Python's `concurrent.futures.ThreadPoolExecutor` to download and extract multiple files concurrently, speeding up the process.

### Organizing Data into Namewise CSV Files

After downloading and extracting the data, the next step is to organize the files into individual CSV files for each weather station. This is done using the following R script:

```r
# Load necessary libraries
library(data.table)

# Set the main directory containing subdirectories by year
main_directory <- "/run/media/pupperemeritus/Seagate Expansion Drive/gsod_archive_parallel"

# Define the desired column names for the output CSV files
desired_column_names <- c(
  "STATION", "DATE", "LATITUDE", "LONGITUDE", "ELEVATION", "NAME",
  "TEMP", "TEMP_ATTRIBUTES", "DEWP", "DEWP_ATTRIBUTES", "SLP",
  "SLP_ATTRIBUTES", "STP", "STP_ATTRIBUTES", "VISIB", "VISIB_ATTRIBUTES",
  "WDSP", "WDSP_ATTRIBUTES", "MXSPD", "GUST", "MAX", "MAX_ATTRIBUTES",
  "MIN", "MIN_ATTRIBUTES", "PRCP", "PRCP_ATTRIBUTES", "SNDP", "FRSHTT"
)

# Get a list of all subdirectories (years) in the main directory
subdirectories <- list.dirs(main_directory, recursive = FALSE)

# Loop through each subdirectory (year)
for (subdir in subdirectories) {
  # Log progress
  cat("Processing files in directory:", subdir, "\n")

  # Set the working directory to the current subdirectory
  setwd(subdir)

  # Get a list of all CSV files in the current subdirectory
  csv_files <- list.files(pattern = "*.csv")

  # Loop through each CSV file in the current subdirectory
  for (file in csv_files) {
    # Log progress
    cat("Processing file:", file, "\n")

    # Read the CSV file into a data table
    data <- read.csv(file, header = TRUE, sep = ",")

    # Get unique station names from the data
    unique_stations <- unique(data$NAME)

    # Loop through each unique station
    for (station in unique_stations) {
      # Subset the data for the current station
      station_data <- data[data$NAME == station, ]

      # Define the output file name based on the station name
      output_file <- file.path("/home/pupperemeritus/minorproject2/combined_data/", paste0(station, ".csv"))

      # Create the output directory if it doesn't exist
      if (!dir.exists(dirname(output_file))) {
        dir.create(dirname(output_file), recursive = TRUE)
      }

      # Write the station data to the output file with desired column names
      write.table(station_data, file = output_file, row.names = FALSE, col.names = FALSE, append = TRUE)
    }
  }

  # Log progress
  cat("Completed processing files in directory:", subdir, "\n")
}

# Log completion of the entire script
cat("All files processed successfully.\n")

# Set the working directory back to the main directory after processing all subdirectories
setwd(main_directory)
```

This R script does the following:

1. **Set Main Directory**: Specifies the main directory containing subdirectories for each year.
2. **Process Each Subdirectory**: Iterates through each year's directory, processing all CSV files within.
3. **Organize Data by Station**: For each CSV file, extracts data for each unique weather station and writes it to a separate CSV file named after the station.

### Combining Data for Indian Stations

Finally, a separate R script is used to combine data for Indian stations into a single CSV file:

```r
library(dplyr)

# Get the list of files in the directory
files <- list.files("indian_stations_output", full.names = TRUE)

# Initialize an empty data frame to hold the combined data
combined_data <- data.frame()
desired_column_names <- c(
  "STATION", "DATE", "LATITUDE", "LONGITUDE", "ELEVATION", "NAME",
  "TEMP", "TEMP_ATTRIBUTES", "DEWP", "DEWP_ATTRIBUTES", "SLP",
  "SLP_ATTRIBUTES", "STP", "STP_ATTRIBUTES", "VISIB", "VISIB_ATTRIBUTES",
  "WDSP", "WDSP_ATTRIBUTES", "MXSPD", "GUST", "MAX", "MAX_ATTRIBUTES",
  "MIN", "MIN_ATTRIBUTES", "PRCP", "PRCP_ATTRIBUTES", "SNDP", "FRSHTT"
)

# Loop through each file and read it into a data frame
for (file in files) {
  # Read the file into a data frame
  data <- read.csv(file, sep = " ", header = FALSE)
  print(data)
  # Append the data to the combined data frame
  combined_data <- bind_rows(combined_data, data)
}

# Write the combined data to a csv file
write.csv(combined_data, "combined_data.csv", sep = ",", row.names = FALSE, col.names = desired_column_names)
```

This script performs the following tasks:

1. **List Files**: Retrieves all files in the directory containing data for Indian stations.
2. **Combine Data**: Reads each file and combines the data into a single data frame.
3. **Write Combined Data**: Outputs the combined data frame to a CSV file named `combined_data.csv`.

These steps ensure that the data is downloaded, extracted, and organized efficiently for further analysis.## Converting Date Column

The `DATE` column, which contains the date of the observation, is initially read as a character string. We convert it to a date format using the `as.Date` function. This is important for any time series analysis or operations that require date manipulation.

```r
weather_data$DATE <- as.Date(weather_data$DATE)
```

## Extracting Day of the Year

To analyze seasonal patterns, we extract the day of the year (ranging from 1 to 365 or 366 for leap years) from the `DATE` column using the `yday` function from the `lubridate` package.

```r
weather_data$DAY_OF_YEAR <- yday(weather_data$DATE)
```

## Identifying Numeric Columns

The dataset contains several columns representing different weather attributes, such as temperature, dew point, wind speed, etc. We identify these columns and ensure they are in numeric format, which is necessary for performing statistical calculations.

```r
numeric_cols <- c("TEMP_SI", "DEWP_SI", "WDSP_SI", "MXSPD_SI", "MAX_SI", "MIN_SI", "PRCP_SI", "SNDP_SI")
weather_data[numeric_cols] <- lapply(weather_data[numeric_cols], as.numeric)
```

## Handling Missing Values

Missing values in the dataset can cause errors in analysis and visualization. We handle missing values by removing rows with NA values in any of the numeric columns. Alternatively, more sophisticated methods such as imputation could be used, but for simplicity, we remove these rows.

```r
weather_data <- weather_data %>%
    filter(complete.cases(weather_data[numeric_cols]))
```

### Calculating Mean and Standard Deviation

We create a lookup table that contains the mean and standard deviation for each day of the year and each station.

```r
mean_sd_lookup <- weather_data %>%
    group_by(DAY_OF_YEAR, NAME) %>%
    summarise(
        TEMP_SI_mean = mean(TEMP_SI, na.rm = TRUE),
        TEMP_SI_sd = sd(TEMP_SI, na.rm = TRUE),
        DEWP_SI_mean = mean(DEWP_SI, na.rm = TRUE),
        DEWP_SI_sd = sd(DEWP_SI, na.rm = TRUE),
        WDSP_SI_mean = mean(WDSP_SI, na.rm = TRUE),
        WDSP_SI_sd = sd(WDSP_SI, na.rm = TRUE),
        MXSPD_SI_mean = mean(MXSPD_SI, na.rm = TRUE),
        MXSPD_SI_sd = sd(MXSPD_SI, na.rm = TRUE),
        MAX_SI_mean = mean(MAX_SI, na.rm = TRUE),
        MAX_SI_sd = sd(MAX_SI, na.rm = TRUE),
        MIN_SI_mean = mean(MIN_SI, na.rm = TRUE),
        MIN_SI_sd = sd(MIN_SI, na.rm = TRUE),
        PRCP_SI_mean = mean(PRCP_SI, na.rm = TRUE),
        PRCP_SI_sd = sd(PRCP_SI, na.rm = TRUE),
        SNDP_SI_mean = mean(SNDP_SI, na.rm = TRUE),
        SNDP_SI_sd = sd(SNDP_SI, na.rm = TRUE),
        .groups = "drop"
    )
```


# Summary of preprocessing

This detailed preprocessing section covered downloading the data, converting and formatting date columns, and handling missing values. These steps ensure that the data is clean and ready for subsequent analysis and visualization.



### Interactive Plots and Their Descriptions

#### 1. Weather Station Analysis

This interactive plot allows users to visualize and analyze weather data from different stations on a map, select specific stations, and view detailed statistics.

**Code:**

```r
library(shiny)
library(leaflet)
library(dplyr)
library(lubridate)

# Load data outside server for better performance
weather_data <- read.csv("./data/cleaned_data.csv")
station_data <- NULL

# UI
ui <- fluidPage(
  titlePanel("Weather Station Analysis"),

  # Leaflet map
  leafletOutput("map"),

  # Dropdown menu for selecting weather stations
  selectInput("station", "Select Station:", choices = NULL),

  # Display station details
  verbatimTextOutput("station_details"),

  # Display statistics
  verbatimTextOutput("stats")
)

# Server
server <- function(input, output, session) {
  # Initialize map
  output$map <- renderLeaflet({
    leaflet() %>%
      addProviderTiles("CartoDB.Positron") %>%
      setView(lng = 78.9629, lat = 20.5937, zoom = 5)
  })

  # Update station dropdown choices
  observe({
    updateSelectInput(session, "station", choices = unique(weather_data$NAME))
  })

  # Filter data for selected station
  selected_station <- reactive({
    req(input$station)
    filter(weather_data, NAME == input$station)
  })

  # Update map marker when station is selected
  observe({
    station <- selected_station()
    if (nrow(station) > 0) {
      leafletProxy("map") %>%
        clearMarkers() %>%
        addMarkers(
          data = station,
          lng = ~LONGITUDE,
          lat = ~LATITUDE,
          popup = ~ paste(
            "Station:", STATION, "<br>",
            "Name:", NAME, "<br>",
            "Elevation:", ELEVATION, "<br>",
            "Temperature:", TEMP_SI, "°C"
          ),
          label = ~ paste("Avg Temp:", round(mean(TEMP_SI, na.rm = TRUE), 2), "°C"),
          labelOptions = labelOptions(direction = "auto")
        )
    }
  })

  # Calculate average temperature and most occurrence of FRSHTT type year-wise
  output$stats <- renderPrint({
    station <- selected_station()
    if (nrow(station) > 0) {
      stats <- station %>%
        group_by(year = year(DATE)) %>%
        summarise(
          Avg_Temperature = mean(TEMP_SI, na.rm = TRUE),
          Most_FRSHTT = names(which.max(table(FRSHTT)))
        )
      print(stats)
    } else {
      "No data available for selected station."
    }
  })
}

# Run the application
shinyApp(ui = ui, server = server)
```

**Description:**

- **Map Visualization**: The application initializes a Leaflet map centered over India.
- **Station Selection**: Users can select a weather station from a dropdown menu, which dynamically updates the available choices based on the loaded data.
- **Marker Display**: When a station is selected, a marker appears on the map at the station's location, displaying details such as station name, elevation, and average temperature.
- **Statistics Display**: The application calculates and displays average yearly temperature and the most frequent weather condition (FRSHTT) for the selected station.

#### 2. Seasonal Weather Clustering

This interactive plot clusters weather stations based on seasonal weather data and visualizes them on a map, allowing users to explore different clusters for each season.

**Code:**

```r
library(shiny)
library(cluster)
library(dplyr)
library(leaflet)
library(shinydashboard)
library(htmlwidgets)

# Load data
weather_data <- read.csv("./data/cleaned_data.csv")

# Convert DATE column to Date type
weather_data$DATE <- as.Date(weather_data$DATE)

# Extract season information from dates (assuming seasons are defined as quarters)
weather_data$Season <- cut(
  as.POSIXlt(weather_data$DATE)$mon + 1, # Adding 1 because months are indexed from 0
  breaks = c(0, 2, 3, 6, 9, 11, 12),
  labels = c("Winter", "Spring", "Summer", "Monsoon", "Fall", "Winter"), # Include Winter twice for full cycle
  include.lowest = TRUE
)

# Determine the number of clusters (you can change this)
num_clusters <- 6

# Create a color palette for clusters
cluster_colors <- rainbow(num_clusters)

createSeasonMap <- function(season_name, zoom_level = 2) {
  # Filter data for the specified season
  season_data <- weather_data %>% filter(Season == season_name)

  # Group by Station and calculate average values
  grouped_data <- season_data %>%
    group_by(STATION) %>%
    summarize(
      AVG_TEMP_SI = mean(TEMP_SI, na.rm = TRUE),
      AVG_PRCP_SI = mean(PRCP_SI, na.rm = TRUE),
      AVG_ELEVATION = mean(ELEVATION, na.rm = TRUE),
      LONGITUDE = first(LONGITUDE), # Add longitude column
      LATITUDE = first(LATITUDE) # Add latitude column
    ) %>%
    ungroup()

  # Scale the data
  scaled_data <- scale(grouped_data[, c("AVG_TEMP_SI", "AVG_PRCP_SI", "AVG_ELEVATION")])

  # Perform k-means clustering
  kmeans_result <- kmeans(scaled_data, centers = num_clusters)

  # Add cluster assignments to the grouped data
  grouped_data$Cluster <- kmeans_result$cluster

  # Calculate cluster sizes for adjusting zoom level
  cluster_sizes <- table(kmeans_result$cluster)
  max_cluster_size <- max(cluster_sizes)
  zoom_adjustment <- ceiling(log10(max_cluster_size)) # Adjust zoom based on cluster size

  # Create a leaflet map to visualize the clustered data for this season
  map <- leaflet(data = grouped_data) %>%
    addTiles() %>%
    addCircleMarkers(
      radius = 5,
      color = cluster_colors[grouped_data$Cluster],
      fillOpacity = 0.8, # Adjust fill opacity
      # clusterOptions = markerClusterOptions()
    ) %>%
    addLegend(
      "bottomright",
      colors = cluster_colors,
      labels = paste("Cluster", 1:num_clusters),
      opacity = 1
    ) %>%
    setView(lng = mean(grouped_data$LONGITUDE), lat = mean(grouped_data$LATITUDE), zoom = zoom_level + zoom_adjustment) %>%
    # Add JavaScript code to handle marker click events and show popup with details
    onRender("
      function(map) {
        this.on('click', function(event) {
          var marker = event.layer;
          marker.bindPopup(
            '<b>Station:</b> ' + marker.options.title + '<br>' +
            '<b>Avg Temperature:</b> ' + marker.options.avg_temp + '<br>' +
            '<b>Avg Precipitation:</b> ' + marker.options.avg_prcp + '<br>' +
            '<b>Avg Elevation:</b> ' + marker.options.avg_elevation
          ).openPopup();
        });
      }
    ")
  for (i in 1:nrow(grouped_data)) {
    map <- map %>%
      addCircleMarkers(
        lng = grouped_data$LONGITUDE[i],
        lat = grouped_data$LATITUDE[i],
        radius = 5,
        color = cluster_colors[grouped_data$Cluster[i]],
        fillOpacity = 0.8,
        popup = paste(
          "<b>Station:</b> ", grouped_data$STATION[i], "<br>",
          "<b>Avg Temperature:</b> ", round(grouped_data$AVG_TEMP_SI[i], 2), "<br>",
          "<b>Avg Precipitation:</b> ", round(grouped_data$AVG_PRCP_SI[i], 2), "<br>",
          "<b>Avg Elevation:</b> ", round(grouped_data$AVG_ELEVATION[i], 2)
        )
      )
  }

  return(map)
}

# Define UI for the application
ui <- dashboardPage(
  skin = "blue",
  dashboardHeader(
    title = "Seasonal Weather Clustering",
    titleWidth = 600
  ),
  dashboardSidebar(
    width = 200,
    sidebarMenu(
      id = "tabs",
      menuItem("Winter", tabName = "winter", icon = icon("snowflake")), # Updated icon names
      menuItem("Spring", tabName = "spring", icon = icon("tree")), # Updated icon names
      menuItem("Summer", tabName = "summer", icon = icon("sun")), # Updated icon names
      menuItem("Monsoon", tabName = "monsoon", icon = icon("cloud-rain")), # Updated icon names
      menuItem("Fall", tabName = "fall", icon = icon("leaf")) # Updated icon names
    )
  ),
  dashboardBody(
    tags$style(
      HTML("
      .content-wrapper, .right-side {
        margin-left: 200px; /* Sidebar width */
      }
      .main-sidebar {
        width: 200px;
      }
      .leaflet-container {
        height: 95vh !important;
        width: calc(100vw - 210px) !important; /* Full width minus sidebar width */
        margin: 

0 auto;
        padding: 0;
      }
      ")
    ),
    tabItems(
      tabItem(
        tabName = "winter",
        leafletOutput("map_winter", width = "100%", height = "100%")
      ),
      tabItem(
        tabName = "spring",
        leafletOutput("map_spring", width = "100%", height = "100%")
      ),
      tabItem(
        tabName = "summer",
        leafletOutput("map_summer", width = "100%", height = "100%")
      ),
      tabItem(
        tabName = "fall",
        leafletOutput("map_fall", width = "100%", height = "100%")
      ),
      tabItem(
        tabName = "monsoon",
        leafletOutput("map_monsoon", width = "100%", height = "100%")
      )
    )
  )
)

# Define server logic
server <- function(input, output) {
  output$map_winter <- renderLeaflet({
    createSeasonMap("Winter")
  })
  output$map_spring <- renderLeaflet({
    createSeasonMap("Spring")
  })
  output$map_summer <- renderLeaflet({
    createSeasonMap("Summer")
  })
  output$map_monsoon <- renderLeaflet({
    createSeasonMap("Monsoon")
  })
  output$map_fall <- renderLeaflet({
    createSeasonMap("Fall")
  })
}

# Run the application
shinyApp(ui = ui, server = server)
```

**Description:**

- **Seasonal Data**: The application divides the weather data into seasons and clusters the stations based on average temperature, precipitation, and elevation.
- **Cluster Visualization**: Each season's data is visualized on a map with markers representing different clusters. Each marker provides a popup with detailed information about the station's average temperature, precipitation, and elevation.
- **Interactive Map**: Users can switch between different seasons using the sidebar menu, and the map dynamically updates to show the clusters for the selected season.

#### 3. Weather Forecasting App

This interactive plot provides temperature forecasts for a selected weather station based on a SARIMA model.

**Code:**

```r
library(shiny)
library(astsa)
library(dplyr)

# Load your cleaned data
weather_data <- read.csv("cleaned_data.csv")

# Define UI
ui <- fluidPage(
  titlePanel("Weather Forecasting App"),
  sidebarLayout(
    sidebarPanel(
      selectInput("station", "Select Station:",
        choices = unique(weather_data$NAME)
      ),
      numericInput("forecast_days", "Number of Forecast Days:", value = 7, min = 1, max = 30),
      actionButton("submit", "Submit")
    ),
    mainPanel(
      plotOutput("forecast_plot")
    )
  )
)

server <- function(input, output) {
  observeEvent(input$submit, {
    # Filter data based on selected station
    selected_station_data <- filter(weather_data, NAME == input$station)

    if (nrow(selected_station_data) == 0) {
      # Handle case where no data is available for the selected station
      # You can show an error message or take appropriate action
      return(NULL)
    }

    # Check for missing values in the temperature data
    if (any(is.na(selected_station_data$TEMP_SI))) {
      # Handle missing values (e.g., impute or remove)
      # Here's a simple example of imputation with mean
      selected_station_data$TEMP_SI[is.na(selected_station_data$TEMP_SI)] <- mean(selected_station_data$TEMP, na.rm = TRUE)
    }

    # Convert date to time series object
    weather_ts <- ts(selected_station_data$TEMP_SI, frequency = 365)

    # Fit SARIMA model using astsa package
    # Adjust seasonal lag if needed (e.g., s = 12 for monthly data)
    sarima_model <- arima(weather_ts, order = c(1, 0, 1), seasonal = list(order = c(1, 0, 1), period = 12))

    # Forecast
    forecast_values <- forecast(sarima_model, h = input$forecast_days)

    # Plot forecast
    output$forecast_plot <- renderPlot({
      plot(forecast_values, main = "Temperature Forecast", xlab = "Date", ylab = "Temperature")
    })
  })
}

# Run the application
shinyApp(ui = ui, server = server)
```

**Description:**

- **Station Selection**: Users select a weather station from a dropdown menu.
- **Forecast Input**: Users input the number of days for which they want the temperature forecast.
- **Forecast Generation**: The application fits a SARIMA model to the selected station's temperature data and generates a forecast for the specified number of days.
- **Forecast Visualization**: The forecasted temperatures are plotted, providing a visual representation of the predicted temperature trends.

These interactive plots leverage the capabilities of Shiny and Leaflet to provide users with a rich, interactive experience for exploring weather data, clustering, and forecasting.


### Documentation for Exploratory Plots

This documentation describes the exploratory plots generated from the weather data. These plots include a faceted density plot, a temperature distribution map across India, and a time series analysis of temperature data.

#### 1. Faceted Density Plot of Temperature by Weather Attribute

**Objective:**

The purpose of this plot is to visualize the distribution of temperatures associated with different weather attributes (e.g., fog, rain, snow).

**Code:**

```{r}
library(ggplot2)
library(viridis)
library(tidyr)

# Load data
weather_data <- read.csv("../data/cleaned_data.csv")

# Combine the attributes into a single dataframe
weather_attributes <- c("Fog", "Rain_Drizzle", "Snow_Ice_Pellets", "Hail", "Thunder", "Tornado_Funnel_Cloud")
combined_data <- cbind(weather_data["TEMP_SI"], sapply(weather_attributes, function(attr) factor(weather_data[[attr]])))

# Reshape the data for plotting
combined_data_long <- tidyr::pivot_longer(combined_data, cols = -TEMP_SI, names_to = "Attribute", values_to = "Presence")

# Faceted density plot of temperature by weather attributes with discrete color scale
density_plot <- ggplot(combined_data_long, aes(x = TEMP_SI, fill = Presence)) +
  geom_density(alpha = 0.5) +
  facet_wrap(~Attribute, scales = "free") + # Facet by weather attributes
  labs(
    title = "Density Plot of Temperature by Weather Attribute",
    x = "Temperature (°C)",
    y = "Density"
  ) +
  scale_fill_viridis_d(option = "D") + # Use Viridis color palette
  theme_minimal()

# Print the plot
print(density_plot)
```

**Description:**

- **Data Preparation**: The weather data is combined with weather attributes, reshaped into a long format for plotting.
- **Plotting**: The density plot shows the distribution of temperatures for each weather attribute, with facets for each attribute and colors indicating the presence or absence of the attribute.
- **Color Scale**: Uses the Viridis discrete color palette for better visual distinction and accessibility.

#### 2. Temperature Distribution Across India

**Objective:**

To visualize the spatial distribution of temperature readings across India.

**Code:**

```{r}
#library(ggplot2)

# Load data
#weather_data <- read.csv("./data/cleaned_data.csv")

# Plot temperature values on a map of India with a contrast gradient
ggplot(weather_data, aes(x = LONGITUDE, y = LATITUDE, color = TEMP_SI)) +
  geom_point() +
  labs(
    title = "Temperature Distribution Across India",
    x = "Longitude",
    y = "Latitude",
    color = "Temperature (°C)"
  ) +
  scale_color_gradientn(colors = c("blue", "green", "red")) # Contrast gradient color scale
```

**Description:**

- **Data**: Longitude, latitude, and temperature data points are plotted to show the geographic distribution of temperatures.
- **Color Scale**: A gradient color scale (blue to red) represents the temperature values, providing a visual contrast between different temperature ranges.
- **Output**: The plot is saved as "india_map.png" with specified dimensions.

#### 3. Time Series Plot of Temperature

**Objective:**

To analyze the temporal trend of temperature data over time.

**Code:**

```{r}
# library(ggplot2)

# Load data
# weather_data <- read.csv("./data/cleaned_data.csv")

# Time series analysis
ggplot(weather_data, aes(x = DATE, y = TEMP_SI)) +
  geom_line() +
  labs(
    title = "Time Series Plot of Temperature",
    x = "Date",
    y = "Temperature (°C)"
  )

```

**Description:**

- **Data**: Plots temperature against date to show how temperature changes over time.
- **Visualization**: A line plot is used to represent the continuous nature of the time series data.
- **Output**: The plot is saved as "time_series_plot.png" with specified dimensions.

In this documentation, we've covered a comprehensive journey from data preprocessing to interactive applications for weather analysis. Here's a summary of the key stages and components:

1. **Data Preprocessing:**
   - The preprocessing stage involved fetching weather data from NOAA archives, extracting and organizing it by year and station, and combining stations into CSV files based on station names.
   - Additionally, the data underwent cleaning and formatting to ensure consistency and usability for analysis.

2. **Exploratory Data Analysis (EDA):**
   - The EDA phase included interactive plots created using R packages such as `ggplot2`, `leaflet`, and `viridis`.
   - A faceted density plot depicted temperature distribution by various weather attributes, providing insights into temperature variations under different weather conditions.
   - A temperature distribution map across India showcased the geographic spread of temperature readings, highlighting regional variations.
   - A time series plot illustrated the temporal trends of temperature data, aiding in understanding long-term temperature fluctuations.

3. **Interactive Applications:**
   - The documentation further delved into interactive applications developed using Shiny, a web application framework in R.
   - The Shiny applications included features such as selecting weather stations, visualizing temperature distributions on maps, and forecasting temperature trends.
   - Users could interactively explore weather data, analyze patterns, and generate forecasts tailored to specific stations and timeframes.

4. **Conclusion:**
   - The entire process, from data preprocessing to interactive apps, demonstrated a comprehensive approach to weather data analysis and visualization.
   - Through various plots, maps, and interactive features, users gained insights into temperature patterns, weather attributes, and geographic distributions.
   - These analyses and applications serve as valuable tools for researchers, analysts, and stakeholders involved in weather-related studies, decision-making, and planning.

Overall, this documentation encapsulates the journey of transforming raw weather data into actionable insights and interactive tools, showcasing the power of R programming and data visualization techniques in understanding and leveraging complex datasets for meaningful analysis.